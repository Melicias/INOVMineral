{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "#from numba import jit, cuda\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "#from xgboost import plot_tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#import wget\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gzip\n",
    "import joblib\n",
    "import pydot\n",
    "\n",
    "random_state=42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import randn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from tabgan.sampler import OriginalGenerator, GANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http.request.method_0', 'http.request.method_1', 'http.request.method_2', 'http.request.method_3', 'http.request.method_4', 'http.request.method_5', 'http.request.method_6', 'http.request.method_7', 'http.referer_0', 'http.referer_1', 'http.referer_2', 'http.referer_3', 'http.request.version_0', 'http.request.version_1', 'http.request.version_2', 'http.request.version_3', 'http.request.version_4', 'http.request.version_5', 'http.request.version_6', 'http.request.version_7', 'dns.qry.name.len_0', 'dns.qry.name.len_1', 'dns.qry.name.len_2', 'dns.qry.name.len_3', 'dns.qry.name.len_4', 'dns.qry.name.len_5', 'dns.qry.name.len_6', 'dns.qry.name.len_7', 'dns.qry.name.len_8', 'dns.qry.name.len_9', 'mqtt.conack.flags_0', 'mqtt.conack.flags_1', 'mqtt.conack.flags_2', 'mqtt.conack.flags_3', 'mqtt.conack.flags_4', 'mqtt.conack.flags_5', 'mqtt.conack.flags_6', 'mqtt.protoname_0', 'mqtt.protoname_1', 'mqtt.protoname_2', 'mqtt.topic_0', 'mqtt.topic_1', 'mqtt.topic_2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melicias/.local/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('../data/EdgeIIot_test.csv', low_memory=False)\n",
    "df_train = pd.read_csv('../data/DNN-EdgeIIoT-dataset_SMALL.csv', low_memory=False)\n",
    "\n",
    "drop_columns = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\"arp.dst.proto_ipv4\", \n",
    "         \"http.file_data\",\"http.request.full_uri\",\"icmp.transmit_timestamp\",\n",
    "         \"http.request.uri.query\", \"tcp.options\",\"tcp.payload\",\"tcp.srcport\",\n",
    "         \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n",
    "\n",
    "df_train.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "df_train.drop([\"Attack_label\"], axis=1, inplace=True)\n",
    "df_test.drop([\"Attack_label\"], axis=1, inplace=True)\n",
    "\n",
    "features = [ col for col in df_train.columns if col not in [\"Attack_label\"]+[\"Attack_type\"]] \n",
    "\n",
    "featuresFromStart = [ col for col in df_train.columns if col not in [\"Attack_type\"]]\n",
    "categorical_columns = []\n",
    "for col in df_train.columns[df_train.dtypes == object]:\n",
    "    if col != \"Attack_type\":\n",
    "        categorical_columns.append(col)\n",
    "        \n",
    "catIndexs = []\n",
    "for cc in categorical_columns:\n",
    "    catIndexs.append(featuresFromStart.index(cc))\n",
    "    \n",
    "\n",
    "df = pd.concat([df_train,df_test],keys=[0,1])\n",
    "\n",
    "colunas_one_hot = {}\n",
    "for coluna in categorical_columns:\n",
    "    codes, uniques = pd.factorize(df[coluna].unique())\n",
    "    colunas_one_hot[coluna] = {\"uniques\": uniques, \"codes\":codes}\n",
    "    df[coluna] = df[coluna].replace(colunas_one_hot[coluna][\"uniques\"], colunas_one_hot[coluna][\"codes\"])\n",
    "\n",
    "df = pd.get_dummies(data=df, columns=categorical_columns)\n",
    "\n",
    "df_train,df_test = df.xs(0),df.xs(1)\n",
    "\n",
    "features = [ col for col in df_train.columns if col not in [\"Attack_label\"]+[\"Attack_type\"]] \n",
    "\n",
    "categorical_num = [item for item in features if item not in featuresFromStart]\n",
    "print(categorical_num)\n",
    "\n",
    "catIndexs_num = []\n",
    "for cc in categorical_num:\n",
    "    catIndexs_num.append(features.index(cc))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "features = [ col for col in df_train.columns if col not in [\"Attack_label\"]+[\"Attack_type\"]] \n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train[\"Attack_type\"].values)\n",
    "\n",
    "X_train = df_train[features].values\n",
    "y_train = df_train[\"Attack_type\"].values\n",
    "y_train = le.transform(y_train)\n",
    "\n",
    "X_test = df_test[features].values\n",
    "y_test = df_test[\"Attack_type\"].values\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "standScaler = StandardScaler()\n",
    "model_norm = standScaler.fit(X_train)\n",
    "\n",
    "X_train = model_norm.transform(X_train)\n",
    "X_test = model_norm.transform(X_test)\n",
    "\n",
    "\"\"\"\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train[\"Attack_type\"].values)\n",
    "\n",
    "y_train = df_train[\"Attack_type\"].values\n",
    "df_y_train = pd.DataFrame(le.transform(y_train), columns=[\"Attack_type\"])\n",
    "df_train.drop([\"Attack_type\"], axis=1, inplace=True)\n",
    "\n",
    "X_test = df_test[features].values\n",
    "y_test = df_test[\"Attack_type\"].values\n",
    "\n",
    "df_test_x = pd.DataFrame(X_test, columns=features)\n",
    "df_y_test = pd.DataFrame(le.transform(y_test), columns=[\"Attack_type\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http.request.method_0', 'http.request.method_1', 'http.request.method_2', 'http.request.method_3', 'http.request.method_4', 'http.request.method_5', 'http.request.method_6', 'http.request.method_7', 'http.referer_0', 'http.referer_1', 'http.referer_2', 'http.referer_3', 'http.request.version_0', 'http.request.version_1', 'http.request.version_2', 'http.request.version_3', 'http.request.version_4', 'http.request.version_5', 'http.request.version_6', 'http.request.version_7', 'dns.qry.name.len_0', 'dns.qry.name.len_1', 'dns.qry.name.len_2', 'dns.qry.name.len_3', 'dns.qry.name.len_4', 'dns.qry.name.len_5', 'dns.qry.name.len_6', 'dns.qry.name.len_7', 'dns.qry.name.len_8', 'dns.qry.name.len_9', 'mqtt.conack.flags_0', 'mqtt.conack.flags_1', 'mqtt.conack.flags_2', 'mqtt.conack.flags_3', 'mqtt.conack.flags_4', 'mqtt.conack.flags_5', 'mqtt.conack.flags_6', 'mqtt.protoname_0', 'mqtt.protoname_1', 'mqtt.protoname_2', 'mqtt.topic_0', 'mqtt.topic_1', 'mqtt.topic_2']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5447335a65e342c290794349dd13aa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting CTGAN transformers for each column:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309004f275144d5eb03f09deb9d14684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training CTGAN, epochs::   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "#new_train1, new_target1 = OriginalGenerator().generate_data_pipe(\n",
    "#    pd.DataFrame(X_train, columns=features), pd.DataFrame(y_train, columns=[\"Attack_type\"]), pd.DataFrame(X_test, columns=features), )\n",
    "print(categorical_num)\n",
    "new_train2, new_target2 = GANGenerator(cat_cols=categorical_num).generate_data_pipe(df_train, df_y_train, df_test_x, )\n",
    "\n",
    "# example with all params defined\n",
    "\"\"\"\n",
    "categorical_columns\n",
    "new_train3, new_target3 = GANGenerator(gen_x_times=0.5, cat_cols=categorical_columns,\n",
    "           bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=True,\n",
    "           adversarial_model_params={\n",
    "               \"metrics\": \"AUC\", \"max_depth\": 2, \"max_bin\": 50, \n",
    "               \"learning_rate\": 0.05, \"random_state\": 42, \"n_estimators\": 100,\n",
    "           }, pregeneration_frac=2, only_generated_data=False,\n",
    "           gan_params = {\"batch_size\": 64, \"patience\": 25, \"epochs\" : 200,}).generate_data_pipe(df_train, df_y_train, df_test_x,\n",
    "                                                            deep_copy=True, only_adversarial=False, use_adversarial=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generate_real_samples(n):\n",
    "    X = df_train.sample(n)\n",
    "    y = X[\"Attack_type\"]\n",
    "    X.drop([\"Attack_type\"], axis=1, inplace=True)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim, n_outputs):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=latent_dim, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Dense(n_outputs, activation='softmax')) \n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def define_discriminator(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def plot_history(d_hist, g_hist):\n",
    "    # plot loss\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.plot(d_hist, label='d')\n",
    "    plt.plot(g_hist, label='gen')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_150 (Dense)            (None, 32)                3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_50 (B (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_51 (B (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 15)                975       \n",
      "=================================================================\n",
      "Total params: 6,543\n",
      "Trainable params: 6,351\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator1 = define_generator(X_train.shape[1], len(le.classes_))\n",
    "generator1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_153 (Dense)            (None, 64)                1024      \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 95)                3135      \n",
      "=================================================================\n",
      "Total params: 6,239\n",
      "Trainable params: 6,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator1 = define_discriminator(len(le.classes_),X_train.shape[1])\n",
    "discriminator1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=200):\n",
    "    # determine half the size of one batch, for updating the  discriminator\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_history = []\n",
    "    g_history = []\n",
    "    # manually enumerate epochs\n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        # prepare real samples\n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        print(np.shape(x_real),np.shape(y_real))\n",
    "        print(d_model.summary())\n",
    "        # update discriminator\n",
    "        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real)\n",
    "        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan)\n",
    "        print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f' % (epoch+1, d_loss_real, d_loss_fake, d_loss,  g_loss_fake))\n",
    "        d_history.append(d_loss)\n",
    "        g_history.append(g_loss_fake)\n",
    "        \n",
    "    plot_history(d_history, g_history)    \n",
    "    g_model.save('trained_generated_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "15\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 64)                6144      \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 15)                495       \n",
      "=================================================================\n",
      "Total params: 8,719\n",
      "Trainable params: 8,719\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_159 (Dense)            (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_52 (B (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_53 (B (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 95)                6175      \n",
      "=================================================================\n",
      "Total params: 9,183\n",
      "Trainable params: 8,991\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "(64, 95) (64, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 64)                6144      \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 15)                495       \n",
      "=================================================================\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "Total params: 17,438\n",
      "Trainable params: 8,719\n",
      "Non-trainable params: 8,719\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (64, 1) was passed for an output of shape (None, 15) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/jhkdvwcs203dngzv6h1j8c080000gn/T/ipykernel_26380/3060811875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/d_/jhkdvwcs203dngzv6h1j8c080000gn/T/ipykernel_26380/4115573537.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(g_model, d_model, gan_model, latent_dim, n_epochs, n_batch, n_eval)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# update discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_real_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0md_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_fake_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1175\u001b[0;31m         x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2438\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2440\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2442\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    511\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m                            \u001b[0;34m'This loss expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                            \u001b[0;34m'targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (64, 1) was passed for an output of shape (None, 15) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1])\n",
    "print(len(le.classes_)) # 15\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = X_train.shape[1] #95\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(latent_dim, len(le.classes_))\n",
    "discriminator.summary()\n",
    "# create the generator\n",
    "generator = define_generator(len(le.classes_), latent_dim)\n",
    "generator.summary()\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, len(le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "403cae82c009a3fa8c7225ee8c7440f13d443f49c1cbc18ad9299b24ad5472fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
