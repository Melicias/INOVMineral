---
Lines: 830035
Columns: 159 
Missing value or NaN: 0
---
Categorical columns: 
['type']

--- Details for categorical columns ---
type: 
['udp_flood' 'mqtt_flood' 'http_flood' 'normal' 'tcp_flood'
 'http_flood_node_red' 'icmp_flood' 'port_scanning' 'arp_spoofing'
 'http_botnet']

    Data Type               Column Name  \
0     float64                  duration   
1     float64                orig_bytes   
2     float64                resp_bytes   
3     float64              missed_bytes   
4     float64                 orig_pkts   
5     float64             orig_ip_bytes   
6     float64                 resp_pkts   
7     float64             resp_ip_bytes   
8     float64             flow_duration   
9     float64              fwd_pkts_tot   
10    float64              bwd_pkts_tot   
11    float64         fwd_data_pkts_tot   
12    float64         bwd_data_pkts_tot   
13    float64          fwd_pkts_per_sec   
14    float64          bwd_pkts_per_sec   
15    float64         flow_pkts_per_sec   
16    float64             down_up_ratio   
17    float64       fwd_header_size_tot   
18    float64       bwd_header_size_tot   
19    float64        fwd_PSH_flag_count   
20    float64        bwd_PSH_flag_count   
21    float64       flow_ACK_flag_count   
22    float64      fwd_pkts_payload.min   
23    float64      fwd_pkts_payload.max   
24    float64      fwd_pkts_payload.tot   
25    float64      fwd_pkts_payload.avg   
26    float64      fwd_pkts_payload.std   
27    float64      bwd_pkts_payload.min   
28    float64      bwd_pkts_payload.max   
29    float64      bwd_pkts_payload.tot   
30    float64      bwd_pkts_payload.avg   
31    float64      bwd_pkts_payload.std   
32    float64     flow_pkts_payload.min   
33    float64     flow_pkts_payload.max   
34    float64     flow_pkts_payload.tot   
35    float64     flow_pkts_payload.avg   
36    float64     flow_pkts_payload.std   
37    float64               fwd_iat.min   
38    float64               fwd_iat.max   
39    float64               fwd_iat.tot   
40    float64               fwd_iat.avg   
41    float64               fwd_iat.std   
42    float64               bwd_iat.min   
43    float64               bwd_iat.max   
44    float64               bwd_iat.tot   
45    float64               bwd_iat.avg   
46    float64               bwd_iat.std   
47    float64              flow_iat.min   
48    float64              flow_iat.max   
49    float64              flow_iat.tot   
50    float64              flow_iat.avg   
51    float64              flow_iat.std   
52    float64  payload_bytes_per_second   
53    float64          fwd_subflow_pkts   
54    float64          bwd_subflow_pkts   
55    float64         fwd_subflow_bytes   
56    float64         bwd_subflow_bytes   
57    float64            fwd_bulk_bytes   
58    float64            bwd_bulk_bytes   
59    float64          fwd_bulk_packets   
60    float64          bwd_bulk_packets   
61    float64             fwd_bulk_rate   
62    float64             bwd_bulk_rate   
63    float64                active.max   
64    float64                active.tot   
65    float64                active.avg   
66    float64                active.std   
67    float64                  idle.min   
68    float64                  idle.max   
69    float64                  idle.tot   
70    float64                  idle.avg   
71    float64                  idle.std   
72    float64      fwd_init_window_size   
73    float64      bwd_init_window_size   
74    float64      fwd_last_window_size   
75    float64      bwd_last_window_size   
76    float64                proto_icmp   
77    float64                 proto_tcp   
78    float64                 proto_udp   
79    float64            conn_state_OTH   
80    float64            conn_state_REJ   
81    float64           conn_state_RSTO   
82    float64         conn_state_RSTOS0   
83    float64           conn_state_RSTR   
84    float64          conn_state_RSTRH   
85    float64             conn_state_S0   
86    float64             conn_state_S1   
87    float64             conn_state_S2   
88    float64             conn_state_S3   
89    float64             conn_state_SF   
90    float64             conn_state_SH   
91    float64            conn_state_SHR   
92    float64     fwd_header_size_min_0   
93    float64     fwd_header_size_min_8   
94    float64    fwd_header_size_min_20   
95    float64    fwd_header_size_min_24   
96    float64    fwd_header_size_min_32   
97    float64    fwd_header_size_min_40   
98    float64    fwd_header_size_min_44   
99    float64     fwd_header_size_max_0   
100   float64     fwd_header_size_max_8   
101   float64    fwd_header_size_max_20   
102   float64    fwd_header_size_max_24   
103   float64    fwd_header_size_max_32   
104   float64    fwd_header_size_max_40   
105   float64    fwd_header_size_max_44   
106   float64     bwd_header_size_min_0   
107   float64     bwd_header_size_min_8   
108   float64    bwd_header_size_min_20   
109   float64    bwd_header_size_min_24   
110   float64    bwd_header_size_min_32   
111   float64    bwd_header_size_min_40   
112   float64    bwd_header_size_min_44   
113   float64     bwd_header_size_max_0   
114   float64     bwd_header_size_max_8   
115   float64    bwd_header_size_max_20   
116   float64    bwd_header_size_max_24   
117   float64    bwd_header_size_max_32   
118   float64    bwd_header_size_max_40   
119   float64    bwd_header_size_max_44   
120   float64    bwd_header_size_max_52   
121   float64     flow_FIN_flag_count_0   
122   float64     flow_FIN_flag_count_1   
123   float64     flow_FIN_flag_count_2   
124   float64     flow_FIN_flag_count_3   
125   float64     flow_FIN_flag_count_4   
126   float64     flow_FIN_flag_count_5   
127   float64     flow_FIN_flag_count_6   
128   float64     flow_FIN_flag_count_7   
129   float64     flow_SYN_flag_count_0   
130   float64     flow_SYN_flag_count_1   
131   float64     flow_SYN_flag_count_2   
132   float64     flow_SYN_flag_count_3   
133   float64     flow_SYN_flag_count_4   
134   float64     flow_SYN_flag_count_5   
135   float64     flow_SYN_flag_count_6   
136   float64     flow_SYN_flag_count_7   
137   float64     flow_SYN_flag_count_8   
138   float64     flow_SYN_flag_count_9   
139   float64    flow_SYN_flag_count_10   
140   float64     flow_RST_flag_count_0   
141   float64     flow_RST_flag_count_1   
142   float64     flow_RST_flag_count_2   
143   float64     flow_RST_flag_count_3   
144   float64     flow_RST_flag_count_4   
145   float64      history_originator_0   
146   float64      history_originator_1   
147   float64      history_originator_2   
148   float64      history_originator_3   
149   float64      history_originator_4   
150   float64      history_originator_5   
151   float64      history_originator_6   
152   float64       history_responder_0   
153   float64       history_responder_1   
154   float64       history_responder_2   
155   float64       history_responder_3   
156   float64       history_responder_4   
157   float64       history_responder_5   
158    object                      type   

                                         Unique Values  
0    [-0.0146794852089584, -0.4057007074806505, -0....  
1    [-0.1798294178884204, -0.1798284914145006, 5.0...  
2    [-0.0050909125602386, -0.0050749461149235, -0....  
3                                           [0.0, 1.0]  
4    [-0.0442144921158877, -0.3029477249158591, 0.9...  
5    [-0.2235051422612724, -0.2701244830698417, -0....  
6    [-0.7565662370859079, 0.113379129033401, 2.723...  
7    [-0.6513299229855699, 0.0761837856163627, 0.01...  
8    [-0.0146794852089584, -0.4057007074806505, -0....  
9    [-0.0442144921158877, -0.3029477249158591, 0.9...  
10   [-0.7565662370859079, 0.113379129033401, 2.723...  
11   [-0.1841505084302839, 1.8849776192465009, 2.91...  
12   [-0.0468426670141985, 15.094668954347926, 30.2...  
13   [-0.7784400498089381, -0.7784161600321544, -0....  
14   [-0.7778685962767037, -0.777780086048685, 1.31...  
15   [-0.7784992390061136, -0.7784872839061784, -0....  
16   [-1.303155859508906, 0.7080096245472669, 0.037...  
17   [-0.5632548105483247, -0.4759722062381908, -0....  
18   [-0.5899833859035155, 0.0606276880264669, -0.0...  
19   [-0.1847802841859509, 1.9741081084026069, 3.05...  
20   [-0.0400030915305518, 17.674125540826267, 35.3...  
21   [-0.4998346498111727, -0.0539362824453553, 3.5...  
22   [-0.0715485643139634, 3.915326526805518, 4.733...  
23   [-0.2240566597355144, 1.7390343359970422, 2.41...  
24   [-0.1700283834593249, 0.4782412227926198, 1.49...  
25   [-0.202207993037164, 0.5625309784999971, 2.751...  
26   [-0.2180729836878303, 1.5162121242004258, 3.06...  
27   [-0.0268542124667472, 35.75915639532796, 32.97...  
28   [-0.0415997877172178, 19.401657524859345, 8.83...  
29   [-0.0425570558754673, 18.57474482657504, 8.452...  
30   [-0.0348008605996162, 33.5905000562465, 2.5224...  
31   [-0.0298206347677357, 8.991496232710693, 8.322...  
32   [-0.0331878993021227, 11.642130649671818, 14.0...  
33   [-0.2246240054143394, 1.7370633135516742, 2.40...  
34   [-0.1706558309600847, 0.4767949484126132, 1.49...  
35   [-0.2080860613189242, 0.646748724811026, 2.935...  
36   [-0.2211732043688052, 1.350672247020747, 3.018...  
37   [0.7127558178183264, -0.1241863594563378, -0.4...  
38   [0.2733864256711472, -0.3354093518719633, -0.5...  
39   [-0.0028152123596323, -0.3933972387175649, -0....  
40   [0.5051920576769914, -0.2519777043187228, -0.5...  
41   [-0.3664930315787462, -0.0066253488630917, 1.4...  
42   [-0.0943335427477205, -0.0942693179403854, 0.2...  
43   [-0.1919051737110198, 2.452781667564118, 0.523...  
44   [-0.136594453953829, 0.572018528282717, 0.0948...  
45   [-0.1760385217376787, 1.2357883632501545, 0.51...  
46   [-0.1701881005640612, 3.1116935451146284, 0.68...  
47   [0.7007557821325137, -0.1404231255835309, -0.3...  
48   [0.2580938529997307, -0.355182492360928, -0.52...  
49   [-0.0147052155493456, -0.4057263732713586, -0....  
50   [0.4947340760258622, -0.2672872228429325, -0.4...  
51   [-0.3717570839796863, -0.1029017127743767, -0....  
52   [-0.0167643314864417, -0.0162993341521579, -0....  
53   [-0.1664665308562612, 4.051102316774855, -1.22...  
54   [-1.2294417626206369, 0.6387953628886931, 2.50...  
55   [-0.211129749304483, 1.0516171041470712, 3.039...  
56   [-0.0335462033978086, 21.33447097684775, 2.404...  
57   [-0.0045563503109509, 234.63874932947647, 138....  
58   [-0.003094060504225, 333.5791545891124, 50.847...  
59   [-0.0046737429031634, 216.44925318646543, 180....  
60           [-0.0033419406315067, 299.22733832322217]  
61   [-0.0022212486057844, 0.0106504297139593, 50.1...  
62   [-0.0014985658322627, 0.4679980432883466, 0.72...  
63   [-0.2104019440618189, 0.5913780130804039, -0.0...  
64   [-0.2110921674416496, 0.5694058858568176, -0.0...  
65   [-0.2074104452541365, 0.6109421180480645, -0.0...  
66   [-0.0579698713082919, -0.0555156831535085, -0....  
67   [0.5670191944053457, -0.4771993911044491, -0.0...  
68   [0.2791260775230978, -0.5365049285270181, -0.2...  
69   [0.0220513888074027, -0.5105013742139537, -0.3...  
70   [0.428915493265397, -0.5282022665843201, -0.17...  
71   [-0.3039416991711032, 1.8771591824375828, 2.35...  
72   [-0.8016621040909139, -0.7851928581929801, 1.2...  
73   [-0.7083135508908369, 1.4079016290762738, 1.43...  
74   [-0.7251696633681214, -0.7082262240141926, 1.4...  
75   [-0.6538613341045789, 1.5307894976787302, 1.52...  
76                                          [0.0, 1.0]  
77                                          [0.0, 1.0]  
78                                          [1.0, 0.0]  
79                                          [0.0, 1.0]  
80                                          [0.0, 1.0]  
81                                          [0.0, 1.0]  
82                                          [0.0, 1.0]  
83                                          [0.0, 1.0]  
84                                          [0.0, 1.0]  
85                                          [1.0, 0.0]  
86                                          [0.0, 1.0]  
87                                          [0.0, 1.0]  
88                                          [0.0, 1.0]  
89                                          [0.0, 1.0]  
90                                          [0.0, 1.0]  
91                                          [0.0, 1.0]  
92                                          [0.0, 1.0]  
93                                          [1.0, 0.0]  
94                                          [0.0, 1.0]  
95                                          [0.0, 1.0]  
96                                          [0.0, 1.0]  
97                                          [0.0, 1.0]  
98                                          [0.0, 1.0]  
99                                          [0.0, 1.0]  
100                                         [1.0, 0.0]  
101                                         [0.0, 1.0]  
102                                         [0.0, 1.0]  
103                                         [0.0, 1.0]  
104                                         [0.0, 1.0]  
105                                         [0.0, 1.0]  
106                                         [1.0, 0.0]  
107                                         [0.0, 1.0]  
108                                         [0.0, 1.0]  
109                                         [0.0, 1.0]  
110                                         [0.0, 1.0]  
111                                         [0.0, 1.0]  
112                                         [0.0, 1.0]  
113                                         [1.0, 0.0]  
114                                         [0.0, 1.0]  
115                                         [0.0, 1.0]  
116                                         [0.0, 1.0]  
117                                         [0.0, 1.0]  
118                                         [0.0, 1.0]  
119                                         [0.0, 1.0]  
120                                         [0.0, 1.0]  
121                                         [1.0, 0.0]  
122                                         [0.0, 1.0]  
123                                         [0.0, 1.0]  
124                                         [0.0, 1.0]  
125                                         [0.0, 1.0]  
126                                         [0.0, 1.0]  
127                                         [0.0, 1.0]  
128                                         [0.0, 1.0]  
129                                         [1.0, 0.0]  
130                                         [0.0, 1.0]  
131                                         [0.0, 1.0]  
132                                         [0.0, 1.0]  
133                                         [0.0, 1.0]  
134                                         [0.0, 1.0]  
135                                         [0.0, 1.0]  
136                                         [0.0, 1.0]  
137                                         [0.0, 1.0]  
138                                         [0.0, 1.0]  
139                                              [0.0]  
140                                         [1.0, 0.0]  
141                                         [0.0, 1.0]  
142                                         [0.0, 1.0]  
143                                         [0.0, 1.0]  
144                                         [0.0, 1.0]  
145                                         [0.0, 1.0]  
146                                         [1.0, 0.0]  
147                                         [0.0, 1.0]  
148                                         [0.0, 1.0]  
149                                         [0.0, 1.0]  
150                                         [0.0, 1.0]  
151                                         [0.0, 1.0]  
152                                         [1.0, 0.0]  
153                                         [0.0, 1.0]  
154                                         [0.0, 1.0]  
155                                         [0.0, 1.0]  
156                                         [0.0, 1.0]  
157                                         [0.0, 1.0]  
158  [udp_flood, mqtt_flood, http_flood, normal, tc...  
epoch 0  | loss: 1.14238 | train_accuracy: 0.63034 | valid_accuracy: 0.63073 |  0:01:09s
epoch 1  | loss: 0.44963 | train_accuracy: 0.75301 | valid_accuracy: 0.75475 |  0:02:18s
epoch 2  | loss: 0.43847 | train_accuracy: 0.79112 | valid_accuracy: 0.79383 |  0:03:27s
epoch 3  | loss: 0.42604 | train_accuracy: 0.80307 | valid_accuracy: 0.80536 |  0:04:36s
epoch 4  | loss: 0.42009 | train_accuracy: 0.80478 | valid_accuracy: 0.80714 |  0:05:46s
epoch 5  | loss: 0.42358 | train_accuracy: 0.80446 | valid_accuracy: 0.80695 |  0:06:55s
epoch 6  | loss: 0.42525 | train_accuracy: 0.80933 | valid_accuracy: 0.81195 |  0:08:05s
epoch 7  | loss: 0.42426 | train_accuracy: 0.80329 | valid_accuracy: 0.80532 |  0:09:15s
epoch 8  | loss: 0.42401 | train_accuracy: 0.80676 | valid_accuracy: 0.80907 |  0:10:24s
epoch 9  | loss: 0.41491 | train_accuracy: 0.80776 | valid_accuracy: 0.80989 |  0:11:34s
epoch 10 | loss: 0.41249 | train_accuracy: 0.80867 | valid_accuracy: 0.8111  |  0:12:43s
epoch 11 | loss: 0.41027 | train_accuracy: 0.80819 | valid_accuracy: 0.81056 |  0:13:53s
epoch 12 | loss: 0.40776 | train_accuracy: 0.8155  | valid_accuracy: 0.81752 |  0:15:03s
epoch 13 | loss: 0.40633 | train_accuracy: 0.81703 | valid_accuracy: 0.81916 |  0:16:13s
epoch 14 | loss: 0.42496 | train_accuracy: 0.8145  | valid_accuracy: 0.81665 |  0:17:24s
epoch 15 | loss: 0.40731 | train_accuracy: 0.81559 | valid_accuracy: 0.8175  |  0:18:33s
epoch 16 | loss: 0.40547 | train_accuracy: 0.81848 | valid_accuracy: 0.82059 |  0:19:42s
epoch 17 | loss: 0.40005 | train_accuracy: 0.81724 | valid_accuracy: 0.81912 |  0:20:52s
epoch 18 | loss: 0.40047 | train_accuracy: 0.81869 | valid_accuracy: 0.82072 |  0:22:01s
epoch 19 | loss: 0.40185 | train_accuracy: 0.81874 | valid_accuracy: 0.82093 |  0:23:10s
epoch 20 | loss: 0.40279 | train_accuracy: 0.81539 | valid_accuracy: 0.81757 |  0:24:18s
epoch 21 | loss: 0.40087 | train_accuracy: 0.81609 | valid_accuracy: 0.81825 |  0:25:27s
epoch 22 | loss: 0.39838 | train_accuracy: 0.81843 | valid_accuracy: 0.82048 |  0:26:37s
epoch 23 | loss: 0.40017 | train_accuracy: 0.81863 | valid_accuracy: 0.82062 |  0:27:45s
epoch 24 | loss: 0.40002 | train_accuracy: 0.81915 | valid_accuracy: 0.82113 |  0:28:54s
epoch 25 | loss: 0.39773 | train_accuracy: 0.81645 | valid_accuracy: 0.81844 |  0:30:02s
epoch 26 | loss: 0.39697 | train_accuracy: 0.81736 | valid_accuracy: 0.81924 |  0:31:10s
epoch 27 | loss: 0.39543 | train_accuracy: 0.819   | valid_accuracy: 0.82127 |  0:32:19s
epoch 28 | loss: 0.3951  | train_accuracy: 0.81766 | valid_accuracy: 0.81948 |  0:33:27s
epoch 29 | loss: 0.39505 | train_accuracy: 0.81798 | valid_accuracy: 0.81986 |  0:34:35s
epoch 30 | loss: 0.39771 | train_accuracy: 0.81912 | valid_accuracy: 0.82109 |  0:35:44s
epoch 31 | loss: 0.39783 | train_accuracy: 0.81889 | valid_accuracy: 0.82081 |  0:36:52s
epoch 32 | loss: 0.39425 | train_accuracy: 0.81739 | valid_accuracy: 0.81944 |  0:38:03s
epoch 33 | loss: 0.39715 | train_accuracy: 0.81687 | valid_accuracy: 0.81899 |  0:39:13s
epoch 34 | loss: 0.39588 | train_accuracy: 0.81663 | valid_accuracy: 0.81846 |  0:40:22s
epoch 35 | loss: 0.40175 | train_accuracy: 0.81783 | valid_accuracy: 0.81984 |  0:41:30s
epoch 36 | loss: 0.39314 | train_accuracy: 0.81915 | valid_accuracy: 0.82116 |  0:42:39s
epoch 37 | loss: 0.38551 | train_accuracy: 0.81806 | valid_accuracy: 0.82021 |  0:43:47s
epoch 38 | loss: 0.3848  | train_accuracy: 0.81556 | valid_accuracy: 0.81769 |  0:44:57s
epoch 39 | loss: 0.38091 | train_accuracy: 0.81706 | valid_accuracy: 0.81916 |  0:46:06s
epoch 40 | loss: 0.3799  | train_accuracy: 0.81506 | valid_accuracy: 0.8166  |  0:47:16s
epoch 41 | loss: 0.3791  | train_accuracy: 0.81749 | valid_accuracy: 0.81942 |  0:48:24s
epoch 42 | loss: 0.37828 | train_accuracy: 0.82338 | valid_accuracy: 0.82504 |  0:49:33s
epoch 43 | loss: 0.37656 | train_accuracy: 0.76041 | valid_accuracy: 0.76134 |  0:50:43s
epoch 44 | loss: 0.37705 | train_accuracy: 0.81854 | valid_accuracy: 0.82034 |  0:51:52s
epoch 45 | loss: 0.37923 | train_accuracy: 0.82105 | valid_accuracy: 0.82298 |  0:53:00s
epoch 46 | loss: 0.38012 | train_accuracy: 0.80305 | valid_accuracy: 0.80442 |  0:54:08s
epoch 47 | loss: 0.37823 | train_accuracy: 0.8225  | valid_accuracy: 0.82431 |  0:55:16s
epoch 48 | loss: 0.37729 | train_accuracy: 0.81573 | valid_accuracy: 0.81774 |  0:56:24s
epoch 49 | loss: 0.37639 | train_accuracy: 0.72533 | valid_accuracy: 0.72651 |  0:57:32s
epoch 50 | loss: 0.37588 | train_accuracy: 0.81788 | valid_accuracy: 0.81977 |  0:58:41s
epoch 51 | loss: 0.37624 | train_accuracy: 0.82285 | valid_accuracy: 0.82483 |  0:59:49s
epoch 52 | loss: 0.37704 | train_accuracy: 0.8138  | valid_accuracy: 0.81607 |  1:00:57s
epoch 53 | loss: 0.3781  | train_accuracy: 0.72477 | valid_accuracy: 0.72596 |  1:02:05s
epoch 54 | loss: 0.37611 | train_accuracy: 0.82173 | valid_accuracy: 0.82372 |  1:03:14s
epoch 55 | loss: 0.37727 | train_accuracy: 0.81798 | valid_accuracy: 0.82018 |  1:04:24s
epoch 56 | loss: 0.37599 | train_accuracy: 0.82097 | valid_accuracy: 0.82289 |  1:05:34s
epoch 57 | loss: 0.38069 | train_accuracy: 0.81515 | valid_accuracy: 0.81693 |  1:06:42s
epoch 58 | loss: 0.37603 | train_accuracy: 0.72729 | valid_accuracy: 0.72848 |  1:07:50s
epoch 59 | loss: 0.37778 | train_accuracy: 0.81841 | valid_accuracy: 0.82063 |  1:08:59s
epoch 60 | loss: 0.37552 | train_accuracy: 0.82109 | valid_accuracy: 0.82287 |  1:10:07s
epoch 61 | loss: 0.37405 | train_accuracy: 0.81897 | valid_accuracy: 0.82112 |  1:11:15s
epoch 62 | loss: 0.37419 | train_accuracy: 0.72475 | valid_accuracy: 0.72584 |  1:12:23s
epoch 63 | loss: 0.37367 | train_accuracy: 0.7264  | valid_accuracy: 0.72773 |  1:13:31s
epoch 64 | loss: 0.37499 | train_accuracy: 0.81854 | valid_accuracy: 0.82056 |  1:14:38s
epoch 65 | loss: 0.37441 | train_accuracy: 0.80054 | valid_accuracy: 0.80223 |  1:15:46s
epoch 66 | loss: 0.3751  | train_accuracy: 0.81654 | valid_accuracy: 0.81872 |  1:16:54s
epoch 67 | loss: 0.37384 | train_accuracy: 0.7215  | valid_accuracy: 0.72276 |  1:18:01s
epoch 68 | loss: 0.37348 | train_accuracy: 0.72575 | valid_accuracy: 0.72692 |  1:19:09s
epoch 69 | loss: 0.37288 | train_accuracy: 0.72279 | valid_accuracy: 0.7241  |  1:20:17s
epoch 70 | loss: 0.37241 | train_accuracy: 0.81712 | valid_accuracy: 0.81891 |  1:21:24s
epoch 71 | loss: 0.37485 | train_accuracy: 0.81713 | valid_accuracy: 0.81895 |  1:22:32s
epoch 72 | loss: 0.37229 | train_accuracy: 0.81692 | valid_accuracy: 0.81892 |  1:23:39s
epoch 73 | loss: 0.37226 | train_accuracy: 0.7219  | valid_accuracy: 0.72323 |  1:24:47s
epoch 74 | loss: 0.37234 | train_accuracy: 0.75871 | valid_accuracy: 0.75891 |  1:25:55s
epoch 75 | loss: 0.37191 | train_accuracy: 0.72412 | valid_accuracy: 0.7254  |  1:27:02s
epoch 76 | loss: 0.37185 | train_accuracy: 0.81718 | valid_accuracy: 0.81893 |  1:28:10s
epoch 77 | loss: 0.3738  | train_accuracy: 0.71714 | valid_accuracy: 0.7188  |  1:29:18s
epoch 78 | loss: 0.37216 | train_accuracy: 0.72338 | valid_accuracy: 0.72472 |  1:30:25s
epoch 79 | loss: 0.37124 | train_accuracy: 0.81937 | valid_accuracy: 0.82144 |  1:31:33s
epoch 80 | loss: 0.37201 | train_accuracy: 0.72521 | valid_accuracy: 0.72636 |  1:32:42s
epoch 81 | loss: 0.37077 | train_accuracy: 0.81827 | valid_accuracy: 0.81993 |  1:33:50s
epoch 82 | loss: 0.37059 | train_accuracy: 0.73944 | valid_accuracy: 0.74057 |  1:34:58s
epoch 83 | loss: 0.37359 | train_accuracy: 0.727   | valid_accuracy: 0.72822 |  1:36:06s
epoch 84 | loss: 0.37123 | train_accuracy: 0.75244 | valid_accuracy: 0.75258 |  1:37:14s
epoch 85 | loss: 0.3695  | train_accuracy: 0.72895 | valid_accuracy: 0.73007 |  1:38:22s
epoch 86 | loss: 0.36828 | train_accuracy: 0.72429 | valid_accuracy: 0.72552 |  1:39:30s
epoch 87 | loss: 0.36965 | train_accuracy: 0.72129 | valid_accuracy: 0.72271 |  1:40:38s
epoch 88 | loss: 0.49313 | train_accuracy: 0.74288 | valid_accuracy: 0.74476 |  1:41:46s
epoch 89 | loss: 0.51892 | train_accuracy: 0.76335 | valid_accuracy: 0.76571 |  1:42:53s
epoch 90 | loss: 0.44574 | train_accuracy: 0.81157 | valid_accuracy: 0.81359 |  1:44:02s
epoch 91 | loss: 0.41443 | train_accuracy: 0.81368 | valid_accuracy: 0.81541 |  1:45:11s
epoch 92 | loss: 0.41566 | train_accuracy: 0.81432 | valid_accuracy: 0.81609 |  1:46:18s
epoch 93 | loss: 0.41133 | train_accuracy: 0.81331 | valid_accuracy: 0.81497 |  1:47:26s
epoch 94 | loss: 0.41011 | train_accuracy: 0.81492 | valid_accuracy: 0.81674 |  1:48:35s
epoch 95 | loss: 0.41079 | train_accuracy: 0.81516 | valid_accuracy: 0.81706 |  1:49:42s
epoch 96 | loss: 0.40728 | train_accuracy: 0.8149  | valid_accuracy: 0.8167  |  1:50:53s
epoch 97 | loss: 0.40636 | train_accuracy: 0.81467 | valid_accuracy: 0.81635 |  1:52:02s
epoch 98 | loss: 0.40605 | train_accuracy: 0.81481 | valid_accuracy: 0.81656 |  1:53:09s
epoch 99 | loss: 0.40687 | train_accuracy: 0.80799 | valid_accuracy: 0.81018 |  1:54:17s
epoch 100| loss: 0.40882 | train_accuracy: 0.81405 | valid_accuracy: 0.81577 |  1:55:24s
epoch 101| loss: 0.40706 | train_accuracy: 0.81546 | valid_accuracy: 0.81727 |  1:56:31s
epoch 102| loss: 0.40436 | train_accuracy: 0.81423 | valid_accuracy: 0.81606 |  1:57:39s
epoch 103| loss: 0.40729 | train_accuracy: 0.81547 | valid_accuracy: 0.81731 |  1:58:46s
epoch 104| loss: 0.40648 | train_accuracy: 0.81561 | valid_accuracy: 0.81748 |  1:59:54s
epoch 105| loss: 0.40346 | train_accuracy: 0.81533 | valid_accuracy: 0.81703 |  2:01:01s
epoch 106| loss: 0.40558 | train_accuracy: 0.81529 | valid_accuracy: 0.81724 |  2:02:08s
epoch 107| loss: 0.40431 | train_accuracy: 0.81596 | valid_accuracy: 0.81786 |  2:03:16s
epoch 108| loss: 0.40211 | train_accuracy: 0.8142  | valid_accuracy: 0.81609 |  2:04:23s
epoch 109| loss: 0.40642 | train_accuracy: 0.81614 | valid_accuracy: 0.81801 |  2:05:32s
epoch 110| loss: 0.39967 | train_accuracy: 0.81564 | valid_accuracy: 0.81753 |  2:06:41s
epoch 111| loss: 0.39505 | train_accuracy: 0.8163  | valid_accuracy: 0.81821 |  2:07:49s
epoch 112| loss: 0.3955  | train_accuracy: 0.81234 | valid_accuracy: 0.81404 |  2:08:56s
epoch 113| loss: 0.40432 | train_accuracy: 0.81626 | valid_accuracy: 0.81802 |  2:10:04s
epoch 114| loss: 0.39284 | train_accuracy: 0.8151  | valid_accuracy: 0.81703 |  2:11:12s
epoch 115| loss: 0.38932 | train_accuracy: 0.81678 | valid_accuracy: 0.81828 |  2:12:19s
epoch 116| loss: 0.38614 | train_accuracy: 0.72118 | valid_accuracy: 0.72208 |  2:13:27s
epoch 117| loss: 0.38471 | train_accuracy: 0.81652 | valid_accuracy: 0.81833 |  2:14:34s
epoch 118| loss: 0.38425 | train_accuracy: 0.81697 | valid_accuracy: 0.81863 |  2:15:44s
epoch 119| loss: 0.3829  | train_accuracy: 0.81645 | valid_accuracy: 0.81827 |  2:16:54s
epoch 120| loss: 0.38425 | train_accuracy: 0.81909 | valid_accuracy: 0.82077 |  2:18:02s
epoch 121| loss: 0.38257 | train_accuracy: 0.8178  | valid_accuracy: 0.81958 |  2:19:10s
epoch 122| loss: 0.38163 | train_accuracy: 0.81537 | valid_accuracy: 0.8172  |  2:20:17s
epoch 123| loss: 0.38096 | train_accuracy: 0.7217  | valid_accuracy: 0.72291 |  2:21:24s
epoch 124| loss: 0.38078 | train_accuracy: 0.81821 | valid_accuracy: 0.81994 |  2:22:32s
epoch 125| loss: 0.38047 | train_accuracy: 0.82166 | valid_accuracy: 0.82336 |  2:23:39s
epoch 126| loss: 0.38321 | train_accuracy: 0.81878 | valid_accuracy: 0.8207  |  2:24:47s
epoch 127| loss: 0.38136 | train_accuracy: 0.72209 | valid_accuracy: 0.72322 |  2:25:54s
epoch 128| loss: 0.38026 | train_accuracy: 0.8214  | valid_accuracy: 0.82312 |  2:27:01s
epoch 129| loss: 0.37937 | train_accuracy: 0.81405 | valid_accuracy: 0.8158  |  2:28:09s
epoch 130| loss: 0.37862 | train_accuracy: 0.72354 | valid_accuracy: 0.72462 |  2:29:17s
epoch 131| loss: 0.37729 | train_accuracy: 0.72357 | valid_accuracy: 0.72476 |  2:30:25s
epoch 132| loss: 0.37763 | train_accuracy: 0.81513 | valid_accuracy: 0.81677 |  2:31:32s
epoch 133| loss: 0.37804 | train_accuracy: 0.72253 | valid_accuracy: 0.72352 |  2:32:39s
epoch 134| loss: 0.37827 | train_accuracy: 0.72429 | valid_accuracy: 0.72529 |  2:33:47s
epoch 135| loss: 0.38042 | train_accuracy: 0.72207 | valid_accuracy: 0.72338 |  2:34:54s
epoch 136| loss: 0.38042 | train_accuracy: 0.81867 | valid_accuracy: 0.82046 |  2:36:02s
epoch 137| loss: 0.37745 | train_accuracy: 0.72269 | valid_accuracy: 0.72392 |  2:37:09s
epoch 138| loss: 0.38065 | train_accuracy: 0.82167 | valid_accuracy: 0.82355 |  2:38:16s
epoch 139| loss: 0.37695 | train_accuracy: 0.72307 | valid_accuracy: 0.72431 |  2:39:24s
epoch 140| loss: 0.3767  | train_accuracy: 0.82189 | valid_accuracy: 0.82368 |  2:40:31s
epoch 141| loss: 0.37611 | train_accuracy: 0.80207 | valid_accuracy: 0.80329 |  2:41:38s
epoch 142| loss: 0.37542 | train_accuracy: 0.82069 | valid_accuracy: 0.82236 |  2:42:46s

Early stopping occurred at epoch 142 with best_epoch = 42 and best_valid_accuracy = 0.82504
----- Time and memory usage -----
(current, peak) (1890281, 6887592620)
--- 9838.47 segundos ---
------------------------------------
--- Performance of tabnet multiclass smote ---
Accuracy : 82.55%
Precision: 87.23%
Recall: 82.55%
F1-score: 82.43%
Balanced accuracy: 73.26%
Classification report:
              precision    recall  f1-score   support

           0       0.25      0.96      0.40       250
           1       0.00      0.00      0.00         1
           2       0.73      0.93      0.82     19782
           3       0.82      0.50      0.62     13428
           4       1.00      1.00      1.00      6029
           5       0.99      0.70      0.82     19130
           6       0.98      0.29      0.45      2310
           7       0.56      0.97      0.71      1117
           8       0.48      0.97      0.64      5282
           9       1.00      1.00      1.00     22209

    accuracy                           0.83     89538
   macro avg       0.68      0.73      0.65     89538
weighted avg       0.87      0.83      0.82     89538

[('duration', 0.0), ('orig_bytes', 0.0), ('resp_bytes', 0.001), ('missed_bytes', 0.0056), ('orig_pkts', 0.0058), ('orig_ip_bytes', 0.0006), ('resp_pkts', 0.0518), ('resp_ip_bytes', 0.0), ('flow_duration', 0.0), ('fwd_pkts_tot', 0.0), ('bwd_pkts_tot', 0.0), ('fwd_data_pkts_tot', 0.0565), ('bwd_data_pkts_tot', 0.0004), ('fwd_pkts_per_sec', 0.0), ('bwd_pkts_per_sec', 0.0683), ('flow_pkts_per_sec', 0.0008), ('down_up_ratio', 0.0028), ('fwd_header_size_tot', 0.0), ('bwd_header_size_tot', 0.0113), ('fwd_PSH_flag_count', 0.0199), ('bwd_PSH_flag_count', 0.0083), ('flow_ACK_flag_count', 0.0), ('fwd_pkts_payload.min', 0.0), ('fwd_pkts_payload.max', 0.0), ('fwd_pkts_payload.tot', 0.0), ('fwd_pkts_payload.avg', 0.0), ('fwd_pkts_payload.std', 0.0), ('bwd_pkts_payload.min', 0.0), ('bwd_pkts_payload.max', 0.0), ('bwd_pkts_payload.tot', 0.0), ('bwd_pkts_payload.avg', 0.0), ('bwd_pkts_payload.std', 0.0015), ('flow_pkts_payload.min', 0.0), ('flow_pkts_payload.max', 0.0), ('flow_pkts_payload.tot', 0.0), ('flow_pkts_payload.avg', 0.0498), ('flow_pkts_payload.std', 0.0004), ('fwd_iat.min', 0.0), ('fwd_iat.max', 0.0001), ('fwd_iat.tot', 0.0198), ('fwd_iat.avg', 0.0113), ('fwd_iat.std', 0.0), ('bwd_iat.min', 0.0), ('bwd_iat.max', 0.0), ('bwd_iat.tot', 0.0), ('bwd_iat.avg', 0.0007), ('bwd_iat.std', 0.0003), ('flow_iat.min', 0.0156), ('flow_iat.max', 0.0001), ('flow_iat.tot', 0.0), ('flow_iat.avg', 0.0357), ('flow_iat.std', 0.0), ('payload_bytes_per_second', 0.0), ('fwd_subflow_pkts', 0.0021), ('bwd_subflow_pkts', 0.0252), ('fwd_subflow_bytes', 0.0371), ('bwd_subflow_bytes', 0.0), ('fwd_bulk_bytes', 0.0), ('bwd_bulk_bytes', 0.0), ('fwd_bulk_packets', 0.0001), ('bwd_bulk_packets', 0.0005), ('fwd_bulk_rate', 0.0), ('bwd_bulk_rate', 0.0), ('active.max', 0.0), ('active.tot', 0.0), ('active.avg', 0.0), ('active.std', 0.0), ('idle.min', 0.0), ('idle.max', 0.0), ('idle.tot', 0.0), ('idle.avg', 0.0105), ('idle.std', 0.0), ('fwd_init_window_size', 0.0058), ('bwd_init_window_size', 0.0073), ('fwd_last_window_size', 0.0278), ('bwd_last_window_size', 0.0), ('proto_icmp', 0.0003), ('proto_tcp', 0.0), ('proto_udp', 0.0), ('conn_state_OTH', 0.057), ('conn_state_REJ', 0.0), ('conn_state_RSTO', 0.001), ('conn_state_RSTOS0', 0.0), ('conn_state_RSTR', 0.0007), ('conn_state_RSTRH', 0.0), ('conn_state_S0', 0.0995), ('conn_state_S1', 0.0055), ('conn_state_S2', 0.0), ('conn_state_S3', 0.0), ('conn_state_SF', 0.0043), ('conn_state_SH', 0.0), ('conn_state_SHR', 0.0), ('fwd_header_size_min_0', 0.0), ('fwd_header_size_min_8', 0.0011), ('fwd_header_size_min_20', 0.0052), ('fwd_header_size_min_24', 0.0), ('fwd_header_size_min_32', 0.0), ('fwd_header_size_min_40', 0.0062), ('fwd_header_size_min_44', 0.0), ('fwd_header_size_max_0', 0.0049), ('fwd_header_size_max_8', 0.005), ('fwd_header_size_max_20', 0.0), ('fwd_header_size_max_24', 0.0), ('fwd_header_size_max_32', 0.0), ('fwd_header_size_max_40', 0.1082), ('fwd_header_size_max_44', 0.0009), ('bwd_header_size_min_0', 0.0), ('bwd_header_size_min_8', 0.0), ('bwd_header_size_min_20', 0.0001), ('bwd_header_size_min_24', 0.0076), ('bwd_header_size_min_32', 0.0001), ('bwd_header_size_min_40', 0.0), ('bwd_header_size_min_44', 0.0), ('bwd_header_size_max_0', 0.0), ('bwd_header_size_max_8', 0.0016), ('bwd_header_size_max_20', 0.0), ('bwd_header_size_max_24', 0.0), ('bwd_header_size_max_32', 0.0), ('bwd_header_size_max_40', 0.0003), ('bwd_header_size_max_44', 0.0), ('bwd_header_size_max_52', 0.0001), ('flow_FIN_flag_count_0', 0.0), ('flow_FIN_flag_count_1', 0.0), ('flow_FIN_flag_count_2', 0.0), ('flow_FIN_flag_count_3', 0.058), ('flow_FIN_flag_count_4', 0.0), ('flow_FIN_flag_count_5', 0.0), ('flow_FIN_flag_count_6', 0.0), ('flow_FIN_flag_count_7', 0.0), ('flow_SYN_flag_count_0', 0.0002), ('flow_SYN_flag_count_1', 0.0), ('flow_SYN_flag_count_2', 0.0526), ('flow_SYN_flag_count_3', 0.0), ('flow_SYN_flag_count_4', 0.0006), ('flow_SYN_flag_count_5', 0.0), ('flow_SYN_flag_count_6', 0.0004), ('flow_SYN_flag_count_7', 0.0016), ('flow_SYN_flag_count_8', 0.0), ('flow_SYN_flag_count_9', 0.0), ('flow_SYN_flag_count_10', 0.0125), ('flow_RST_flag_count_0', 0.0012), ('flow_RST_flag_count_1', 0.0), ('flow_RST_flag_count_2', 0.0), ('flow_RST_flag_count_3', 0.0), ('flow_RST_flag_count_4', 0.0), ('history_originator_0', 0.0), ('history_originator_1', 0.0), ('history_originator_2', 0.0169), ('history_originator_3', 0.0), ('history_originator_4', 0.0), ('history_originator_5', 0.0), ('history_originator_6', 0.0), ('history_responder_0', 0.0145), ('history_responder_1', 0.0213), ('history_responder_2', 0.0), ('history_responder_3', 0.025), ('history_responder_4', 0.0), ('history_responder_5', 0.0)]/home/melicias/anaconda3/envs/tabnet/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu
  warnings.warn(f"Device used : {self.device}")
/home/melicias/anaconda3/envs/tabnet/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)

