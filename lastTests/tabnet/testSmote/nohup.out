---
Lines: 1527737
Columns: 97 
Missing value or NaN: 0
---
Categorical columns: 
['Attack_type']

--- Details for categorical columns ---
Attack_type: 
['Normal' 'DDoS_UDP' 'Password' 'DDoS_TCP' 'Backdoor' 'DDoS_ICMP'
 'Port_Scanning' 'Vulnerability_scanner' 'SQL_injection' 'DDoS_HTTP'
 'Uploading' 'XSS' 'Ransomware' 'MITM' 'Fingerprinting']

   Data Type                Column Name  \
0    float64                 arp.opcode   
1    float64                arp.hw.size   
2    float64              icmp.checksum   
3    float64                icmp.seq_le   
4    float64                icmp.unused   
5    float64        http.content_length   
6    float64              http.response   
7    float64              http.tls_port   
8    float64                    tcp.ack   
9    float64                tcp.ack_raw   
10   float64               tcp.checksum   
11   float64         tcp.connection.fin   
12   float64         tcp.connection.rst   
13   float64         tcp.connection.syn   
14   float64      tcp.connection.synack   
15   float64                  tcp.flags   
16   float64              tcp.flags.ack   
17   float64                    tcp.len   
18   float64                    tcp.seq   
19   float64                 udp.stream   
20   float64             udp.time_delta   
21   float64               dns.qry.name   
22   float64                 dns.qry.qu   
23   float64               dns.qry.type   
24   float64         dns.retransmission   
25   float64     dns.retransmit_request   
26   float64  dns.retransmit_request_in   
27   float64     mqtt.conflag.cleansess   
28   float64              mqtt.conflags   
29   float64              mqtt.hdrflags   
30   float64                   mqtt.len   
31   float64        mqtt.msg_decoded_as   
32   float64               mqtt.msgtype   
33   float64             mqtt.proto_len   
34   float64             mqtt.topic_len   
35   float64                   mqtt.ver   
36   float64                  mbtcp.len   
37   float64             mbtcp.trans_id   
38   float64              mbtcp.unit_id   
39     int64               Attack_label   
40    object                Attack_type   
41     int64      http.request.method_0   
42     int64      http.request.method_1   
43     int64      http.request.method_2   
44     int64      http.request.method_3   
45     int64      http.request.method_4   
46     int64      http.request.method_5   
47     int64      http.request.method_6   
48     int64      http.request.method_7   
49     int64      http.request.method_8   
50     int64             http.referer_0   
51     int64             http.referer_1   
52     int64             http.referer_2   
53     int64             http.referer_3   
54     int64             http.referer_4   
55     int64     http.request.version_0   
56     int64     http.request.version_1   
57     int64     http.request.version_2   
58     int64     http.request.version_3   
59     int64     http.request.version_4   
60     int64     http.request.version_5   
61     int64     http.request.version_6   
62     int64     http.request.version_7   
63     int64     http.request.version_8   
64     int64     http.request.version_9   
65     int64    http.request.version_10   
66     int64    http.request.version_11   
67     int64    http.request.version_12   
68     int64         dns.qry.name.len_0   
69     int64         dns.qry.name.len_1   
70     int64         dns.qry.name.len_2   
71     int64         dns.qry.name.len_3   
72     int64         dns.qry.name.len_4   
73     int64         dns.qry.name.len_5   
74     int64         dns.qry.name.len_6   
75     int64         dns.qry.name.len_7   
76     int64         dns.qry.name.len_8   
77     int64         dns.qry.name.len_9   
78     int64        mqtt.conack.flags_0   
79     int64        mqtt.conack.flags_1   
80     int64        mqtt.conack.flags_2   
81     int64        mqtt.conack.flags_3   
82     int64        mqtt.conack.flags_4   
83     int64        mqtt.conack.flags_5   
84     int64        mqtt.conack.flags_6   
85     int64        mqtt.conack.flags_7   
86     int64        mqtt.conack.flags_8   
87     int64        mqtt.conack.flags_9   
88     int64       mqtt.conack.flags_10   
89     int64       mqtt.conack.flags_11   
90     int64       mqtt.conack.flags_12   
91     int64           mqtt.protoname_0   
92     int64           mqtt.protoname_1   
93     int64           mqtt.protoname_2   
94     int64               mqtt.topic_0   
95     int64               mqtt.topic_1   
96     int64               mqtt.topic_2   

                                        Unique Values  
0                                     [0.0, 2.0, 1.0]  
1                                          [0.0, 6.0]  
2   [0.0, 25274.0, 56236.0, 32598.0, 21835.0, 1449...  
3   [0.0, 57101.0, 15950.0, 19413.0, 41521.0, 3247...  
4                                               [0.0]  
5   [0.0, 277.0, 57.0, 1415.0, 36.0, 1.0, 1465.0, ...  
6                                          [0.0, 1.0]  
7                                               [0.0]  
8   [6.0, 0.0, 91101.0, 3531.0, 5.0, 303.0, 59.0, ...  
9   [2774423095.0, 0.0, 2371715782.0, 2371628212.0...  
10  [313.0, 0.0, 60523.0, 21486.0, 50978.0, 24759....  
11                                         [0.0, 1.0]  
12                                         [0.0, 1.0]  
13                                         [0.0, 1.0]  
14                                         [0.0, 1.0]  
15  [16.0, 0.0, 24.0, 17.0, 18.0, 2.0, 4.0, 25.0, ...  
16                                         [1.0, 0.0]  
17  [0.0, 1440.0, 41.0, 32.0, 14.0, 120.0, 1448.0,...  
18  [59.0, 0.0, 22641331.0, 323260.0, 15.0, 262.0,...  
19  [0.0, 1622034.0, 2594368.0, 1871277.0, 10834.0...  
20  [0.0, 154.0, 255.0, 449.0, 399.0, 368.0, 12.0,...  
21  [0.0, 655220.0, 1022370.0, 2559001.0, 2048021....  
22  [0.0, 688.0, 21.0, 371.0, 398.0, 71.0, 476.0, ...  
23                                              [0.0]  
24                             [0.0, 1.0, 28.0, 12.0]  
25                                         [0.0, 1.0]  
26                                         [0.0, 1.0]  
27                                         [0.0, 1.0]  
28                                         [0.0, 2.0]  
29                     [0.0, 48.0, 16.0, 32.0, 224.0]  
30                             [0.0, 39.0, 12.0, 2.0]  
31                                              [0.0]  
32                         [0.0, 3.0, 1.0, 2.0, 14.0]  
33                                         [0.0, 4.0]  
34                                        [0.0, 24.0]  
35                                         [0.0, 4.0]  
36                  [0.0, 27.0, 6.0, 21.0, 9.0, 17.0]  
37  [0.0, 109.0, 121.0, 120.0, 27.0, 145.0, 59.0, ...  
38                     [0.0, 1.0, 3.0, 2.0, 4.0, 6.0]  
39                                             [0, 1]  
40  [Normal, DDoS_UDP, Password, DDoS_TCP, Backdoo...  
41                                             [0, 1]  
42                                             [1, 0]  
43                                             [0, 1]  
44                                             [0, 1]  
45                                             [0, 1]  
46                                             [0, 1]  
47                                             [0, 1]  
48                                             [0, 1]  
49                                             [0, 1]  
50                                             [0, 1]  
51                                             [1, 0]  
52                                             [0, 1]  
53                                             [0, 1]  
54                                             [0, 1]  
55                                             [0, 1]  
56                                             [1, 0]  
57                                             [0, 1]  
58                                             [0, 1]  
59                                             [0, 1]  
60                                             [0, 1]  
61                                             [0, 1]  
62                                             [0, 1]  
63                                             [0, 1]  
64                                             [0, 1]  
65                                             [0, 1]  
66                                             [0, 1]  
67                                             [0, 1]  
68                                             [0, 1]  
69                                             [1, 0]  
70                                             [0, 1]  
71                                             [0, 1]  
72                                             [0, 1]  
73                                             [0, 1]  
74                                             [0, 1]  
75                                             [0, 1]  
76                                             [0, 1]  
77                                             [0, 1]  
78                                             [0, 1]  
79                                             [1, 0]  
80                                             [0, 1]  
81                                             [0, 1]  
82                                             [0, 1]  
83                                             [0, 1]  
84                                             [0, 1]  
85                                             [0, 1]  
86                                             [0, 1]  
87                                             [0, 1]  
88                                             [0, 1]  
89                                             [0, 1]  
90                                             [0, 1]  
91                                             [0, 1]  
92                                             [1, 0]  
93                                             [0, 1]  
94                                             [0, 1]  
95                                             [1, 0]  
96                                             [0, 1]  
epoch 0  | loss: 0.99452 | train_accuracy: 0.6984  | valid_accuracy: 0.91997 |  0:02:15s
epoch 1  | loss: 0.55026 | train_accuracy: 0.71331 | valid_accuracy: 0.92421 |  0:04:30s
epoch 2  | loss: 0.50428 | train_accuracy: 0.74735 | valid_accuracy: 0.92205 |  0:06:44s
epoch 3  | loss: 0.48381 | train_accuracy: 0.75748 | valid_accuracy: 0.92775 |  0:09:00s
epoch 4  | loss: 0.50892 | train_accuracy: 0.75596 | valid_accuracy: 0.92747 |  0:11:15s
epoch 5  | loss: 0.48113 | train_accuracy: 0.76167 | valid_accuracy: 0.92825 |  0:13:31s
epoch 6  | loss: 0.48464 | train_accuracy: 0.76826 | valid_accuracy: 0.92534 |  0:15:46s
epoch 7  | loss: 0.5013  | train_accuracy: 0.72744 | valid_accuracy: 0.92046 |  0:18:01s
epoch 8  | loss: 0.48936 | train_accuracy: 0.75983 | valid_accuracy: 0.93059 |  0:20:16s
epoch 9  | loss: 0.47609 | train_accuracy: 0.75744 | valid_accuracy: 0.92168 |  0:22:30s
epoch 10 | loss: 0.46985 | train_accuracy: 0.7379  | valid_accuracy: 0.92393 |  0:24:45s
epoch 11 | loss: 0.46624 | train_accuracy: 0.76159 | valid_accuracy: 0.93046 |  0:27:00s
epoch 12 | loss: 0.46529 | train_accuracy: 0.76942 | valid_accuracy: 0.93008 |  0:29:14s
epoch 13 | loss: 0.45033 | train_accuracy: 0.78524 | valid_accuracy: 0.91746 |  0:31:28s
epoch 14 | loss: 0.42525 | train_accuracy: 0.79489 | valid_accuracy: 0.93503 |  0:33:42s
epoch 15 | loss: 0.4196  | train_accuracy: 0.79553 | valid_accuracy: 0.93538 |  0:35:56s
epoch 16 | loss: 0.41489 | train_accuracy: 0.7618  | valid_accuracy: 0.93214 |  0:38:10s
epoch 17 | loss: 0.41428 | train_accuracy: 0.79487 | valid_accuracy: 0.93577 |  0:40:24s
epoch 18 | loss: 0.41276 | train_accuracy: 0.70144 | valid_accuracy: 0.92891 |  0:42:38s
epoch 19 | loss: 0.40984 | train_accuracy: 0.7983  | valid_accuracy: 0.93683 |  0:44:52s
epoch 20 | loss: 0.40494 | train_accuracy: 0.73175 | valid_accuracy: 0.93017 |  0:47:07s
epoch 21 | loss: 0.40563 | train_accuracy: 0.79575 | valid_accuracy: 0.9363  |  0:49:21s
epoch 22 | loss: 0.42997 | train_accuracy: 0.75486 | valid_accuracy: 0.92531 |  0:51:36s
epoch 23 | loss: 0.41749 | train_accuracy: 0.70447 | valid_accuracy: 0.93255 |  0:53:53s
epoch 24 | loss: 0.39824 | train_accuracy: 0.71572 | valid_accuracy: 0.93352 |  0:56:09s
epoch 25 | loss: 0.39749 | train_accuracy: 0.73983 | valid_accuracy: 0.92871 |  0:58:25s
epoch 26 | loss: 0.39631 | train_accuracy: 0.73789 | valid_accuracy: 0.93077 |  1:00:41s
epoch 27 | loss: 0.39283 | train_accuracy: 0.74581 | valid_accuracy: 0.93056 |  1:02:57s
epoch 28 | loss: 0.38577 | train_accuracy: 0.74808 | valid_accuracy: 0.93119 |  1:05:14s
epoch 29 | loss: 0.37955 | train_accuracy: 0.68597 | valid_accuracy: 0.92691 |  1:07:28s
epoch 30 | loss: 0.37875 | train_accuracy: 0.7056  | valid_accuracy: 0.9346  |  1:09:42s
epoch 31 | loss: 0.37683 | train_accuracy: 0.68007 | valid_accuracy: 0.92473 |  1:11:56s
epoch 32 | loss: 0.37729 | train_accuracy: 0.70606 | valid_accuracy: 0.92977 |  1:14:09s
epoch 33 | loss: 0.37763 | train_accuracy: 0.68853 | valid_accuracy: 0.92706 |  1:16:22s
epoch 34 | loss: 0.37644 | train_accuracy: 0.6651  | valid_accuracy: 0.92351 |  1:18:36s
epoch 35 | loss: 0.39645 | train_accuracy: 0.75162 | valid_accuracy: 0.93075 |  1:20:49s
epoch 36 | loss: 0.37929 | train_accuracy: 0.71574 | valid_accuracy: 0.93026 |  1:23:02s
epoch 37 | loss: 0.37676 | train_accuracy: 0.74071 | valid_accuracy: 0.93165 |  1:25:16s
epoch 38 | loss: 0.37762 | train_accuracy: 0.7941  | valid_accuracy: 0.9361  |  1:27:29s
epoch 39 | loss: 0.37536 | train_accuracy: 0.7631  | valid_accuracy: 0.93327 |  1:29:42s
epoch 40 | loss: 0.37511 | train_accuracy: 0.74616 | valid_accuracy: 0.93181 |  1:31:59s
epoch 41 | loss: 0.37482 | train_accuracy: 0.73032 | valid_accuracy: 0.92908 |  1:34:13s
epoch 42 | loss: 0.37449 | train_accuracy: 0.72292 | valid_accuracy: 0.92988 |  1:36:26s
epoch 43 | loss: 0.37437 | train_accuracy: 0.80263 | valid_accuracy: 0.93778 |  1:38:39s
epoch 44 | loss: 0.37425 | train_accuracy: 0.66337 | valid_accuracy: 0.92344 |  1:40:52s
epoch 45 | loss: 0.37569 | train_accuracy: 0.77566 | valid_accuracy: 0.93505 |  1:43:07s
epoch 46 | loss: 0.37481 | train_accuracy: 0.70639 | valid_accuracy: 0.92963 |  1:45:21s
epoch 47 | loss: 0.43032 | train_accuracy: 0.73388 | valid_accuracy: 0.92733 |  1:47:39s
epoch 48 | loss: 0.40052 | train_accuracy: 0.73505 | valid_accuracy: 0.92499 |  1:49:56s
epoch 49 | loss: 0.39819 | train_accuracy: 0.68468 | valid_accuracy: 0.90886 |  1:52:14s
epoch 50 | loss: 0.38806 | train_accuracy: 0.68291 | valid_accuracy: 0.92279 |  1:54:29s
epoch 51 | loss: 0.38418 | train_accuracy: 0.75329 | valid_accuracy: 0.93035 |  1:56:43s
epoch 52 | loss: 0.39295 | train_accuracy: 0.73914 | valid_accuracy: 0.92448 |  1:58:57s
epoch 53 | loss: 0.37841 | train_accuracy: 0.70746 | valid_accuracy: 0.92171 |  2:01:10s
epoch 54 | loss: 0.376   | train_accuracy: 0.70824 | valid_accuracy: 0.92192 |  2:03:22s
epoch 55 | loss: 0.37518 | train_accuracy: 0.79988 | valid_accuracy: 0.93741 |  2:05:34s
epoch 56 | loss: 0.37545 | train_accuracy: 0.69891 | valid_accuracy: 0.9209  |  2:07:46s
epoch 57 | loss: 0.37483 | train_accuracy: 0.68739 | valid_accuracy: 0.91886 |  2:09:59s
epoch 58 | loss: 0.37513 | train_accuracy: 0.80337 | valid_accuracy: 0.93847 |  2:12:12s
epoch 59 | loss: 0.37459 | train_accuracy: 0.79746 | valid_accuracy: 0.93596 |  2:14:26s
epoch 60 | loss: 0.37504 | train_accuracy: 0.79553 | valid_accuracy: 0.93621 |  2:16:43s
epoch 61 | loss: 0.3743  | train_accuracy: 0.72884 | valid_accuracy: 0.92463 |  2:18:56s
epoch 62 | loss: 0.37361 | train_accuracy: 0.80176 | valid_accuracy: 0.93786 |  2:21:14s
epoch 63 | loss: 0.3747  | train_accuracy: 0.69733 | valid_accuracy: 0.92081 |  2:23:27s
epoch 64 | loss: 0.37514 | train_accuracy: 0.76405 | valid_accuracy: 0.93368 |  2:25:41s
epoch 65 | loss: 0.38026 | train_accuracy: 0.72456 | valid_accuracy: 0.92538 |  2:27:54s
epoch 66 | loss: 0.37524 | train_accuracy: 0.69737 | valid_accuracy: 0.91639 |  2:30:08s
epoch 67 | loss: 0.37419 | train_accuracy: 0.73467 | valid_accuracy: 0.92601 |  2:32:22s
epoch 68 | loss: 0.37399 | train_accuracy: 0.70565 | valid_accuracy: 0.91866 |  2:34:35s
epoch 69 | loss: 0.37313 | train_accuracy: 0.78653 | valid_accuracy: 0.93179 |  2:36:50s
epoch 70 | loss: 0.37299 | train_accuracy: 0.73945 | valid_accuracy: 0.92885 |  2:39:04s
epoch 71 | loss: 0.37282 | train_accuracy: 0.79195 | valid_accuracy: 0.93816 |  2:41:17s
epoch 72 | loss: 0.42374 | train_accuracy: 0.76319 | valid_accuracy: 0.93037 |  2:43:32s
epoch 73 | loss: 0.3837  | train_accuracy: 0.74799 | valid_accuracy: 0.92931 |  2:45:49s
epoch 74 | loss: 0.37875 | train_accuracy: 0.76165 | valid_accuracy: 0.92965 |  2:48:04s
epoch 75 | loss: 0.37629 | train_accuracy: 0.73706 | valid_accuracy: 0.92525 |  2:50:22s
epoch 76 | loss: 0.3756  | train_accuracy: 0.74604 | valid_accuracy: 0.92935 |  2:52:38s
epoch 77 | loss: 0.37475 | train_accuracy: 0.73437 | valid_accuracy: 0.92425 |  2:54:53s
epoch 78 | loss: 0.3744  | train_accuracy: 0.79045 | valid_accuracy: 0.9341  |  2:57:08s
epoch 79 | loss: 0.37521 | train_accuracy: 0.75886 | valid_accuracy: 0.93293 |  2:59:21s
epoch 80 | loss: 0.37363 | train_accuracy: 0.7062  | valid_accuracy: 0.91374 |  3:01:35s
epoch 81 | loss: 0.37392 | train_accuracy: 0.80117 | valid_accuracy: 0.93786 |  3:03:49s
epoch 82 | loss: 0.37823 | train_accuracy: 0.71916 | valid_accuracy: 0.91649 |  3:06:04s
epoch 83 | loss: 0.37392 | train_accuracy: 0.74879 | valid_accuracy: 0.92509 |  3:08:19s
epoch 84 | loss: 0.37413 | train_accuracy: 0.73328 | valid_accuracy: 0.9203  |  3:10:33s
epoch 85 | loss: 0.37405 | train_accuracy: 0.76261 | valid_accuracy: 0.92991 |  3:12:47s
epoch 86 | loss: 0.37504 | train_accuracy: 0.71999 | valid_accuracy: 0.91828 |  3:15:01s
epoch 87 | loss: 0.37351 | train_accuracy: 0.71579 | valid_accuracy: 0.91729 |  3:17:16s
epoch 88 | loss: 0.43071 | train_accuracy: 0.71929 | valid_accuracy: 0.9226  |  3:19:30s
epoch 89 | loss: 0.44819 | train_accuracy: 0.6989  | valid_accuracy: 0.9126  |  3:21:46s
epoch 90 | loss: 0.44165 | train_accuracy: 0.75348 | valid_accuracy: 0.93071 |  3:23:59s
epoch 91 | loss: 0.44035 | train_accuracy: 0.75642 | valid_accuracy: 0.93236 |  3:26:10s
epoch 92 | loss: 0.4395  | train_accuracy: 0.7014  | valid_accuracy: 0.92319 |  3:28:21s/home/melicias/anaconda3/envs/tabnet/lib/python3.7/site-packages/imblearn/over_sampling/_smote/base.py:340: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.
  FutureWarning,
/home/melicias/anaconda3/envs/tabnet/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cpu
  warnings.warn(f"Device used : {self.device}")
/home/melicias/anaconda3/envs/tabnet/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!
  warnings.warn(wrn_msg)

epoch 93 | loss: 0.43955 | train_accuracy: 0.68078 | valid_accuracy: 0.91999 |  3:30:33s
epoch 94 | loss: 0.43909 | train_accuracy: 0.76247 | valid_accuracy: 0.93011 |  3:32:46s
epoch 95 | loss: 0.4394  | train_accuracy: 0.75883 | valid_accuracy: 0.9295  |  3:35:01s
epoch 96 | loss: 0.43876 | train_accuracy: 0.7362  | valid_accuracy: 0.93306 |  3:37:12s
epoch 97 | loss: 0.43843 | train_accuracy: 0.756   | valid_accuracy: 0.9285  |  3:39:24s
epoch 98 | loss: 0.43816 | train_accuracy: 0.74461 | valid_accuracy: 0.92448 |  3:41:35s
epoch 99 | loss: 0.43863 | train_accuracy: 0.67625 | valid_accuracy: 0.90875 |  3:43:49s
Stop training because you reached max_epochs = 100 with best_epoch = 58 and best_valid_accuracy = 0.93847
Successfully saved model at modelTabNet.zip
----- Time and memory usage -----
(current, peak) (3629755, 9209336579)
--- 13585.49 segundos ---
------------------------------------
--- Performance of decision tree ---
Accuracy : 93.75%
Precision: 96.24%
Recall: 93.75%
F1-score: 93.67%
Balanced accuracy: 80.67%
Classification report:
              precision    recall  f1-score   support

           0       1.00      0.93      0.96      3859
           1       0.95      0.60      0.74      7744
           2       1.00      0.99      1.00     10777
           3       1.00      0.58      0.73      7993
           4       1.00      1.00      1.00     19708
           5       0.25      0.88      0.39       112
           6       0.74      1.00      0.85        63
           7       1.00      1.00      1.00    218119
           8       0.93      0.17      0.29      8130
           9       0.48      1.00      0.65      3239
          10       0.95      0.92      0.94      1561
          11       0.45      0.90      0.60      8014
          12       0.65      0.50      0.56      5790
          13       0.94      0.85      0.89      7998
          14       0.32      0.77      0.45      2440

    accuracy                           0.94    305547
   macro avg       0.78      0.81      0.74    305547
weighted avg       0.96      0.94      0.94    305547

